{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3vLaLLZt84D7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import pickle\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCekJpma9Di8",
        "outputId": "ebada1af-c9a9-4c8b-c476-c9d189edc673"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                            Sentence       Category  Polarity\n",
            "0               But the staff was so horrible to us.        service  negative\n",
            "1  To be completely fair, the only redeeming fact...           food  positive\n",
            "2  The food is uniformly exceptional, with a very...           food  positive\n",
            "3  Where Gabriela personaly greets you and recomm...        service  positive\n",
            "4  For those that go once and don't enjoy it, all...  miscellaneous  positive\n",
            "Test Aspect Data:\n",
            "                                            Sentence Category  Polarity\n",
            "0                    The bread is top notch as well.     food  positive\n",
            "1  I have to say they have one of the fastest del...  service  positive\n",
            "2        Food is always fresh and hot- ready to eat!     food  positive\n",
            "3      Did I mention that the coffee is OUTSTANDING?     food  positive\n",
            "4  Certainly not the best sushi in New York, howe...     food  positive\n",
            "Trial Aspect Data:\n",
            "                                            Sentence       Category  Polarity\n",
            "0  All the appetizers and salads were fabulous, t...           food  positive\n",
            "1                         And really large portions.           food  positive\n",
            "2             Go inside and you won't want to leave.  miscellaneous  positive\n",
            "3  Save yourself the time and trouble and skip th...  miscellaneous  negative\n",
            "4                                 Service was quick.        service  positive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your training data\n",
        "train_data = pd.read_excel(\"C:/Users/vsing/OneDrive/Desktop/Capstone Project/Capstone_project_Sentiment_Analysis_Aspect_Level/Datasets/Capstone Project Dataset- Restaurent Train 1 category.xlsx\")\n",
        "\n",
        "# Fill missing values in the \"Sentence\" column with empty strings\n",
        "train_data[\"Sentence\"] = train_data[\"Sentence\"].fillna(\"\")\n",
        "\n",
        "# Create a new DataFrame with only the \"Sentence,\" \"Category_A,\" and \"Polarity_A\" columns\n",
        "aspect_data = train_data[[\"Sentence\", \"Category_A\", \"Polarity_A\"]]\n",
        "\n",
        "# Rename the columns for clarity\n",
        "aspect_data.columns = [\"Sentence\", \"Category\", \"Polarity\"]\n",
        "\n",
        "# Print the first few rows of the new dataset\n",
        "print(aspect_data.head())\n",
        "\n",
        "# Load your test and trial data\n",
        "test_data = pd.read_excel(\"C:/Users/vsing/OneDrive/Desktop/Capstone Project/Capstone_project_Sentiment_Analysis_Aspect_Level/Datasets/Capstone Project Dataset- Restaurent Test one category.xlsx\")\n",
        "trail_data = pd.read_excel(\"C:/Users/vsing/OneDrive/Desktop/Capstone Project/Capstone_project_Sentiment_Analysis_Aspect_Level/Datasets/Capstone Project Dataset- Restaurent Trial 1 category.xlsx\")\n",
        "\n",
        "# Fill missing values in the \"Sentence\" column with empty strings\n",
        "test_data[\"Sentence\"] = test_data[\"Sentence\"].fillna(\"\")\n",
        "trail_data[\"Sentence\"] = trail_data[\"Sentence\"].fillna(\"\")\n",
        "\n",
        "# Create new DataFrames with only the \"Sentence,\" \"Category_A,\" and \"Polarity_A\" columns\n",
        "test_aspect_data = test_data[[\"Sentence\", \"Category_A\", \"Polarity_A\"]]\n",
        "trail_aspect_data = trail_data[[\"Sentence\", \"Category_A\", \"Polarity_A\"]]\n",
        "\n",
        "# Rename the columns for clarity\n",
        "test_aspect_data.columns = [\"Sentence\", \"Category\", \"Polarity\"]\n",
        "trail_aspect_data.columns = [\"Sentence\", \"Category\", \"Polarity\"]\n",
        "\n",
        "# Print the first few rows of the new datasets\n",
        "print(\"Test Aspect Data:\")\n",
        "print(test_aspect_data.head())\n",
        "\n",
        "print(\"Trial Aspect Data:\")\n",
        "print(trail_aspect_data.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BudgzbyY9DmE",
        "outputId": "2c982da2-fef1-4637-a587-c29c78f34f29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Aspect Category Mapping:\n",
            "{0: 'ambience', 1: 'food', 2: 'miscellaneous', 3: 'price', 4: 'service'}\n",
            "\n",
            "Sentiment (Polarity) Mapping:\n",
            "{0: 'negative', 1: 'neutral', 2: 'positive'}\n"
          ]
        }
      ],
      "source": [
        "# Combine the training, test, and trial datasets\n",
        "all_data = pd.concat([aspect_data, test_aspect_data, trail_aspect_data], ignore_index=True)\n",
        "\n",
        "# Encode the aspect categories using LabelEncoder\n",
        "label_encoder_category = LabelEncoder()\n",
        "all_data['Category'] = label_encoder_category.fit_transform(all_data['Category'])\n",
        "\n",
        "# Encode sentiment (Polarity) using LabelEncoder\n",
        "label_encoder_sentiment = LabelEncoder()\n",
        "all_data['Polarity'] = label_encoder_sentiment.fit_transform(all_data['Polarity'])\n",
        "\n",
        "# Create dictionaries to map encoded labels back to their original values\n",
        "category_mapping = {label: category for label, category in enumerate(label_encoder_category.classes_)}\n",
        "sentiment_mapping = {label: sentiment for label, sentiment in enumerate(label_encoder_sentiment.classes_)}\n",
        "\n",
        "# Print the mapping information\n",
        "print(\"Aspect Category Mapping:\")\n",
        "print(category_mapping)\n",
        "print(\"\\nSentiment (Polarity) Mapping:\")\n",
        "print(sentiment_mapping)\n",
        "\n",
        "# Tokenize the text\n",
        "max_words = 10000  # You can adjust this based on your dataset\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(all_data['Sentence'])\n",
        "\n",
        "# Convert text to sequences\n",
        "sequences = tokenizer.texts_to_sequences(all_data['Sentence'])\n",
        "\n",
        "# Pad sequences to make them of the same length\n",
        "max_sequence_length = 63  # You can adjust this based on your dataset\n",
        "sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "# Split the data back into train, test, and trial datasets\n",
        "train_sequences = sequences[:len(aspect_data)]\n",
        "test_sequences = sequences[len(aspect_data):len(aspect_data) + len(test_aspect_data)]\n",
        "trial_sequences = sequences[len(aspect_data) + len(test_aspect_data):]\n",
        "\n",
        "# Prepare the target labels\n",
        "train_labels_category = all_data['Category'][:len(aspect_data)]\n",
        "test_labels_category = all_data['Category'][len(aspect_data):len(aspect_data) + len(test_aspect_data)]\n",
        "trial_labels_category = all_data['Category'][len(aspect_data) + len(test_aspect_data):]\n",
        "\n",
        "\n",
        "# Prepare the target sentiment labels\n",
        "train_labels_sentiment = all_data['Polarity'][:len(aspect_data)]\n",
        "test_labels_sentiment = all_data['Polarity'][len(aspect_data):len(aspect_data) + len(test_aspect_data)]\n",
        "trial_labels_sentiment = all_data['Polarity'][len(aspect_data) + len(test_aspect_data):]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46FaCntO9DpN",
        "outputId": "090d0160-7897-4f85-9dcb-af8b0371d048"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Maximum Sequence Length: 63\n"
          ]
        }
      ],
      "source": [
        "# Initialize a variable to store the maximum sequence length\n",
        "max_sequence_length = 0\n",
        "\n",
        "# Iterate through the sentences in your dataset\n",
        "for sentence in all_data['Sentence']:\n",
        "    # Tokenize the sentence\n",
        "    tokens = tokenizer.texts_to_sequences([sentence])[0]\n",
        "\n",
        "    # Update max_sequence_length if the current sentence is longer\n",
        "    if len(tokens) > max_sequence_length:\n",
        "        max_sequence_length = len(tokens)\n",
        "\n",
        "print(\"Maximum Sequence Length:\", max_sequence_length)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qefTbLxxcrGl"
      },
      "source": [
        "BILSTM layers for Category and Run the Epochs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-huhBb19DvD",
        "outputId": "ed2ddb8b-34f5-4005-81b9-ae40082c4475"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "85/85 [==============================] - 32s 230ms/step - loss: 5.6727 - accuracy: 0.4576 - val_loss: 4.2621 - val_accuracy: 0.5081 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "85/85 [==============================] - 18s 211ms/step - loss: 2.7282 - accuracy: 0.8051 - val_loss: 3.1769 - val_accuracy: 0.1677 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "85/85 [==============================] - 18s 206ms/step - loss: 1.6794 - accuracy: 0.9029 - val_loss: 2.5380 - val_accuracy: 0.4668 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "85/85 [==============================] - 18s 207ms/step - loss: 1.1754 - accuracy: 0.9459 - val_loss: 2.1593 - val_accuracy: 0.4881 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "85/85 [==============================] - 18s 209ms/step - loss: 0.8224 - accuracy: 0.9741 - val_loss: 1.7984 - val_accuracy: 0.5920 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "85/85 [==============================] - 18s 209ms/step - loss: 0.6813 - accuracy: 0.9844 - val_loss: 1.5365 - val_accuracy: 0.6809 - lr: 1.0000e-04\n",
            "Epoch 7/100\n",
            "85/85 [==============================] - 18s 208ms/step - loss: 0.6439 - accuracy: 0.9930 - val_loss: 1.4358 - val_accuracy: 0.7222 - lr: 1.0000e-04\n",
            "Epoch 8/100\n",
            "85/85 [==============================] - 18s 213ms/step - loss: 0.6283 - accuracy: 0.9867 - val_loss: 1.4440 - val_accuracy: 0.7309 - lr: 1.0000e-04\n",
            "Epoch 9/100\n",
            "85/85 [==============================] - 18s 216ms/step - loss: 0.6037 - accuracy: 0.9926 - val_loss: 1.4548 - val_accuracy: 0.7384 - lr: 1.0000e-04\n",
            "Epoch 10/100\n",
            "85/85 [==============================] - 18s 209ms/step - loss: 0.5786 - accuracy: 0.9930 - val_loss: 1.4553 - val_accuracy: 0.7397 - lr: 1.0000e-04\n",
            "Epoch 11/100\n",
            "85/85 [==============================] - 19s 227ms/step - loss: 0.5607 - accuracy: 0.9911 - val_loss: 1.4664 - val_accuracy: 0.7497 - lr: 1.0000e-04\n",
            "Epoch 12/100\n",
            "85/85 [==============================] - 18s 211ms/step - loss: 0.5303 - accuracy: 0.9933 - val_loss: 1.4517 - val_accuracy: 0.7459 - lr: 1.0000e-04\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x182b8318210>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define the embedding dimension\n",
        "embedding_dim = 100  # You can adjust this based on your dataset\n",
        "# Create the Bi-LSTM model\n",
        "model = Sequential()\n",
        "\n",
        "# Add an Embedding layer\n",
        "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length))\n",
        "model.add(Dropout(0.2))  # Add dropout after embedding\n",
        "\n",
        "# Add a Bidirectional LSTM layer with dropout\n",
        "model.add(Bidirectional(LSTM(64, return_sequences=True, kernel_regularizer=l2(0.01))))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))  # Add dropout after the first LSTM layer\n",
        "model.add(Bidirectional(LSTM(64, kernel_regularizer=l2(0.01))))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))  # Add dropout after the second LSTM layer\n",
        "\n",
        "# Add a Dense layer for aspect classification\n",
        "num_aspect_categories = len(label_encoder_category.classes_)\n",
        "model.add(Dense(num_aspect_categories, activation='softmax', kernel_regularizer=l2(0.01)))\n",
        "\n",
        "# Compile the model with a custom learning rate\n",
        "custom_optimizer = Adam(learning_rate=0.001)  \n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=custom_optimizer, metrics=['accuracy'])\n",
        "\n",
        "# Implement early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model with learning rate scheduling and early stopping\n",
        "def lr_schedule(epoch):\n",
        "    if epoch < 5:\n",
        "        return 0.001\n",
        "    else:\n",
        "        return 0.0001\n",
        "\n",
        "learning_rate_scheduler = LearningRateScheduler(lr_schedule)\n",
        "model.fit(train_sequences, train_labels_category, validation_data=(test_sequences, test_labels_category),\n",
        "          epochs=100, batch_size=32, callbacks=[early_stopping, learning_rate_scheduler])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save the Category Model and Toknizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\vsing\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import os\n",
        "\n",
        "# Define the directory where you want to save the model and tokenizer\n",
        "save_directory = 'save_model_category_2'  # Change this to your desired folder path\n",
        "\n",
        "# Ensure the directory exists, create it if not\n",
        "os.makedirs(save_directory, exist_ok=True)\n",
        "\n",
        "# Save the Keras model in the specified directory\n",
        "model.save(os.path.join(save_directory, 'my_category_model.h5'))\n",
        "\n",
        "# Save the tokenizer in the specified directory\n",
        "with open(os.path.join(save_directory, 'tokenizer.pickle'), 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Apply on testing dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "y7T08DL_3fWL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tensorflow.keras.models import load_model\n",
        "import pickle\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report\n",
        "import os\n",
        "\n",
        "\n",
        "# Define the directory where your model and tokenizer are saved\n",
        "load_directory = 'save_model_category_2'  # Change this to the directory path\n",
        "\n",
        "# Load the Keras model\n",
        "loaded_model = load_model(os.path.join(load_directory, 'my_category_model.h5'))\n",
        "\n",
        "# Load the tokenizer\n",
        "with open(os.path.join(load_directory, 'tokenizer.pickle'), 'rb') as handle:\n",
        "    loaded_tokenizer = pickle.load(handle)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25/25 [==============================] - 4s 42ms/step\n"
          ]
        }
      ],
      "source": [
        "# Load your test data (assuming you have a DataFrame with a \"Sentence\" column)\n",
        "test_data = pd.read_excel(\"C:/Users/vsing/OneDrive/Desktop/Capstone Project/Capstone_project_Sentiment_Analysis_Aspect_Level/Datasets/Capstone Project Dataset- Restaurent Test one category.xlsx\")  # Replace with the actual path\n",
        "\n",
        "# Preprocess the test data for prediction (tokenization and padding)\n",
        "max_sequence_length = 63  # Same as in your training code\n",
        "sequences = tokenizer.texts_to_sequences(test_data['Sentence'])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(padded_sequences)\n",
        "predicted_labels = predictions.argmax(axis=1)\n",
        "\n",
        "# Convert the predicted labels back to their original category values using the mapping\n",
        "predicted_categories = [category_mapping[label] for label in predicted_labels]\n",
        "\n",
        "# Add the predicted categories to the DataFrame\n",
        "test_data['Predicted_Category'] = predicted_categories\n",
        "\n",
        "# Save the results to an Excel file\n",
        "output_file_path = 'Category_predicted_output.xlsx'  # Replace with the desired output file path\n",
        "test_data.to_excel(output_file_path, index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report for Test Dataset:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     ambience       0.75      0.46      0.57        84\n",
            "         food       0.84      0.77      0.81       367\n",
            "miscellaneous       0.67      0.77      0.72       195\n",
            "        price       0.56      0.27      0.37        33\n",
            "      service       0.55      0.78      0.65       120\n",
            "\n",
            "     accuracy                           0.72       799\n",
            "    macro avg       0.68      0.61      0.62       799\n",
            " weighted avg       0.74      0.72      0.72       799\n",
            "\n",
            "\n",
            "Category Analysis Metrics:\n",
            "Accuracy: 0.7221526908635795\n",
            "Precision: 0.7367312054360522\n",
            "Recall: 0.7221526908635795\n",
            "F1 Score: 0.7193281874314086\n"
          ]
        }
      ],
      "source": [
        "# Actual categories\n",
        "actual_categories = test_data['Category_A']\n",
        "\n",
        "# Create a classification report to calculate precision, recall, and F1 score\n",
        "classification_report_result = classification_report(actual_categories, predicted_categories, target_names=category_mapping.values())\n",
        "\n",
        "# Print the classification report\n",
        "print(\"Classification Report for Test Dataset:\")\n",
        "print(classification_report_result)\n",
        "\n",
        "# Category Analysis Metrics (instead of Sentiment)\n",
        "accuracy_category = metrics.accuracy_score(actual_categories,predicted_categories)\n",
        "precision_category = metrics.precision_score(actual_categories, predicted_categories, average='weighted')\n",
        "recall_category = metrics.recall_score(actual_categories, predicted_categories, average='weighted')\n",
        "f1_category = metrics.f1_score(actual_categories, predicted_categories, average='weighted')\n",
        "\n",
        "print(\"\\nCategory Analysis Metrics:\")\n",
        "print(\"Accuracy:\", accuracy_category)\n",
        "print(\"Precision:\", precision_category)\n",
        "print(\"Recall:\", recall_category)\n",
        "print(\"F1 Score:\", f1_category)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "confusion_matrix_result = confusion_matrix(actual_categories, predicted_categories)\n",
        "\n",
        "# Visualize the confusion matrix as a heatmap\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(confusion_matrix_result, annot=True, fmt='d', cmap='Blues', xticklabels=category_mapping.values(), yticklabels=category_mapping.values())\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix for Category Prediction')\n",
        "plt.show()\n",
        "\n",
        "# Print the confusion matrix as a DataFrame\n",
        "confusion_df = pd.DataFrame(confusion_matrix_result, index=category_mapping.values(), columns=category_mapping.values())\n",
        "print(\"Confusion Matrix for Aspect Category Prediction:\")\n",
        "print(confusion_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Predict the Category on Random Sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 50ms/step\n",
            "Sentence: ['this pizza is great']\n",
            "Predicted Aspect Category: food\n"
          ]
        }
      ],
      "source": [
        "# Define a new sentence you want to predict\n",
        "new_sentence = \"this pizza is great\"\n",
        "\n",
        "# Preprocess the new sentence\n",
        "new_sentence = [new_sentence]  # Convert to a list for consistency with previous data\n",
        "new_sequences = tokenizer.texts_to_sequences(new_sentence)\n",
        "new_sequences = pad_sequences(new_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "# Make predictions using the loaded model\n",
        "predicted_category_label = model.predict(new_sequences)\n",
        "predicted_category_label = np.argmax(predicted_category_label)\n",
        "\n",
        "# Convert the predicted label back to the original category\n",
        "predicted_category = category_mapping[predicted_category_label]\n",
        "\n",
        "# Print the predicted category\n",
        "print(f\"Sentence: {new_sentence}\")\n",
        "print(f'Predicted Aspect Category: {predicted_category}')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
